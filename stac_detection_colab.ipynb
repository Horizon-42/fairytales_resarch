{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STAC Detection on Google Colab\n",
        "\n",
        "这个 notebook 用于在 Colab 上运行 STAC (Situation, Task, Action, Consequence) 检测。\n",
        "\n",
        "## 使用说明\n",
        "\n",
        "1. **挂载 Google Drive**：确保你的 `llm_model` 和 `pre_data_process` 文件夹已经在 Google Drive 中（在项目根目录下）\n",
        "2. **选择模型**：支持 Hugging Face 模型（如 Qwen3）\n",
        "3. **配置参数**：设置输入输出路径和模型参数\n",
        "4. **运行检测**：对单个文件或批量文件进行 STAC 检测\n",
        "\n",
        "## 支持的模型\n",
        "\n",
        "- `Qwen/Qwen2.5-7B-Instruct` （推荐，需要较大内存）\n",
        "- `Qwen/Qwen2.5-3B-Instruct` （较小模型，适合 Colab 免费版）\n",
        "- `Qwen/Qwen2-7B-Instruct`\n",
        "- 其他 Hugging Face 聊天模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第一步：挂载 Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 挂载 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"✓ Google Drive mounted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第二步：安装依赖包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装必要的依赖包\n",
        "%pip install -q transformers torch accelerate sentencepiece\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第三步：配置路径和模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ========== 配置路径 ==========\n",
        "# 修改为你 Google Drive 中项目根目录的路径\n",
        "# 例如：/content/drive/MyDrive/fairytales_resarch\n",
        "# 项目结构应该是：\n",
        "#   fairytales_resarch/\n",
        "#     llm_model/\n",
        "#     pre_data_process/\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/fairytales_resarch\"\n",
        "\n",
        "# 验证路径\n",
        "project_path = Path(PROJECT_ROOT)\n",
        "llm_model_path = project_path / \"llm_model\"\n",
        "pre_data_process_path = project_path / \"pre_data_process\"\n",
        "\n",
        "# 添加项目根目录到 Python 路径（这样 llm_model 和 pre_data_process 都能被导入）\n",
        "if project_path.exists():\n",
        "    sys.path.insert(0, str(project_path))\n",
        "    print(f\"✓ Added project root to path: {project_path}\")\n",
        "    \n",
        "    # 验证子目录是否存在\n",
        "    if llm_model_path.exists():\n",
        "        print(f\"✓ Found llm_model: {llm_model_path}\")\n",
        "    else:\n",
        "        print(f\"⚠ Warning: {llm_model_path} not found\")\n",
        "    \n",
        "    if pre_data_process_path.exists():\n",
        "        print(f\"✓ Found pre_data_process: {pre_data_process_path}\")\n",
        "    else:\n",
        "        print(f\"⚠ Warning: {pre_data_process_path} not found\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: {PROJECT_ROOT} not found. Please check the path.\")\n",
        "\n",
        "# ========== 配置模型 ==========\n",
        "# 选择 Hugging Face 模型\n",
        "# 推荐使用较小的模型（如 3B）如果 Colab 内存不足\n",
        "HF_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"  # 或 \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "HF_DEVICE = \"auto\"  # \"cuda\", \"cpu\", 或 \"auto\"（自动检测）\n",
        "\n",
        "# ========== STAC 分析配置 ==========\n",
        "USE_CONTEXT = True  # 使用完整故事上下文\n",
        "USE_NEIGHBORING_SENTENCES = False  # 使用相邻句子作为辅助上下文\n",
        "\n",
        "# 设置环境变量\n",
        "os.environ[\"LLM_PROVIDER\"] = \"huggingface\"\n",
        "os.environ[\"HF_MODEL\"] = HF_MODEL\n",
        "os.environ[\"HF_DEVICE\"] = HF_DEVICE\n",
        "os.environ[\"HF_TEMPERATURE\"] = \"0.2\"\n",
        "os.environ[\"HF_TOP_P\"] = \"0.9\"\n",
        "os.environ[\"HF_MAX_NEW_TOKENS\"] = \"2048\"\n",
        "\n",
        "print(f\"✓ Configuration set\")\n",
        "print(f\"  Model: {HF_MODEL}\")\n",
        "print(f\"  Device: {HF_DEVICE}\")\n",
        "print(f\"  Use Context: {USE_CONTEXT}\")\n",
        "print(f\"  Use Neighboring Sentences: {USE_NEIGHBORING_SENTENCES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第四步：定义句子分割函数（内嵌代码）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 句子分割函数（从 pre_data_process 复制，避免导入问题）\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    \"\"\"将文本按句子切分，正确处理引号内的对话\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return []\n",
        "    \n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    \n",
        "    sentences = []\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "    \n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "        para = re.sub(r'\\n+', ' ', para)\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "        para_sentences = split_sentences_with_quotes(para)\n",
        "        sentences.extend(para_sentences)\n",
        "    \n",
        "    cleaned_sentences = []\n",
        "    for sent in sentences:\n",
        "        sent = sent.strip()\n",
        "        sent = re.sub(r'^[\\s\\u3000]+|[\\s\\u3000]+$', '', sent)\n",
        "        if sent and len(sent) > 0:\n",
        "            cleaned_sentences.append(sent)\n",
        "    \n",
        "    return cleaned_sentences\n",
        "\n",
        "\n",
        "def split_sentences_with_quotes(text: str) -> List[str]:\n",
        "    \"\"\"智能句子切分，正确处理引号内的对话和破折号\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    \n",
        "    double_quotes = {'\\u0022', '\\u201C', '\\u201D'}\n",
        "    single_quotes = {'\\u0027', '\\u2018', '\\u2019'}\n",
        "    chinese_quote_pairs = {'「': '」', '『': '』'}\n",
        "    \n",
        "    sentences = []\n",
        "    i = 0\n",
        "    current_sentence = []\n",
        "    quote_stack = []\n",
        "    sentence_end_chars = ['。', '！', '？', '.', '!', '?', '؟']\n",
        "    closing_chars = ['」', '』', '\"', \"'\", '\"', \"'\", ')', '）']\n",
        "    dash_chars = ['—', '–']\n",
        "    dash_pattern = '——'\n",
        "    \n",
        "    while i < len(text):\n",
        "        char = text[i]\n",
        "        \n",
        "        if char in chinese_quote_pairs:\n",
        "            quote_stack.append(char)\n",
        "            current_sentence.append(char)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        if char in chinese_quote_pairs.values():\n",
        "            if quote_stack and quote_stack[-1] in chinese_quote_pairs:\n",
        "                expected_close = chinese_quote_pairs[quote_stack[-1]]\n",
        "                if char == expected_close:\n",
        "                    quote_stack.pop()\n",
        "                    current_sentence.append(char)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        if char in double_quotes:\n",
        "            if quote_stack and quote_stack[-1] == 'double':\n",
        "                quote_stack.pop()\n",
        "                current_sentence.append(char)\n",
        "            else:\n",
        "                quote_stack.append('double')\n",
        "                current_sentence.append(char)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        if char in single_quotes:\n",
        "            if quote_stack and quote_stack[-1] == 'single':\n",
        "                quote_stack.pop()\n",
        "                current_sentence.append(char)\n",
        "            else:\n",
        "                quote_stack.append('single')\n",
        "                current_sentence.append(char)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        if char in sentence_end_chars:\n",
        "            current_sentence.append(char)\n",
        "            if not quote_stack:\n",
        "                j = i + 1\n",
        "                while j < len(text) and text[j] in closing_chars:\n",
        "                    current_sentence.append(text[j])\n",
        "                    j += 1\n",
        "                while j < len(text) and text[j] in ' \\t\\n':\n",
        "                    j += 1\n",
        "                \n",
        "                if j < len(text):\n",
        "                    sentence = ''.join(current_sentence).strip()\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                    current_sentence = []\n",
        "                    i = j\n",
        "                    continue\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        current_sentence.append(char)\n",
        "        i += 1\n",
        "    \n",
        "    if current_sentence:\n",
        "        sentence = ''.join(current_sentence).strip()\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "    \n",
        "    return sentences\n",
        "\n",
        "\n",
        "def split_sentences_advanced(text: str) -> List[str]:\n",
        "    \"\"\"高级句子切分方法，处理数字、网址、缩写等\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return []\n",
        "    \n",
        "    protected_map = {}\n",
        "    protected_counter = 0\n",
        "    \n",
        "    def protect(match):\n",
        "        nonlocal protected_counter\n",
        "        placeholder = f\"<PROTECTED_{protected_counter}>\"\n",
        "        protected_map[placeholder] = match.group(0)\n",
        "        protected_counter += 1\n",
        "        return placeholder\n",
        "    \n",
        "    text = re.sub(r'\\d+\\.\\d+', protect, text)\n",
        "    text = re.sub(r'https?://[^\\s]+', protect, text)\n",
        "    text = re.sub(r'www\\.[^\\s]+', protect, text)\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', protect, text)\n",
        "    \n",
        "    abbreviations = [\n",
        "        r'\\bMr\\.', r'\\bMrs\\.', r'\\bMs\\.', r'\\bDr\\.', r'\\bProf\\.',\n",
        "        r'\\bSr\\.', r'\\bJr\\.', r'\\bInc\\.', r'\\bLtd\\.', r'\\bCo\\.',\n",
        "        r'\\betc\\.', r'\\bi\\.e\\.', r'\\be\\.g\\.', r'\\bvs\\.', r'\\bU\\.S\\.',\n",
        "    ]\n",
        "    for abbr in abbreviations:\n",
        "        text = re.sub(abbr, protect, text, flags=re.IGNORECASE)\n",
        "    \n",
        "    sentences = split_sentences(text)\n",
        "    \n",
        "    restored_sentences = []\n",
        "    for sent in sentences:\n",
        "        for placeholder, original in protected_map.items():\n",
        "            sent = sent.replace(placeholder, original)\n",
        "        restored_sentences.append(sent)\n",
        "    \n",
        "    return restored_sentences\n",
        "\n",
        "print(\"✓ Sentence splitting functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第五步：导入 STAC 分析模块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的模块（只需要 llm_model，句子分割函数已在上一步定义）\n",
        "try:\n",
        "    from llm_model.llm_router import LLMConfig\n",
        "    from llm_model.huggingface_client import HuggingFaceConfig\n",
        "    from llm_model.stac_analyzer import STACAnalyzerConfig, analyze_stac\n",
        "    \n",
        "    print(\"✓ All modules imported successfully\")\n",
        "    print(\"  - LLMConfig, HuggingFaceConfig\")\n",
        "    print(\"  - STACAnalyzerConfig, analyze_stac\")\n",
        "    print(\"  - split_sentences_advanced (defined in previous cell)\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"✗ Import error: {e}\")\n",
        "    print()\n",
        "    print(\"Troubleshooting:\")\n",
        "    print(\"1. Make sure you ran the 'Configuration' cell (step 3) before this one\")\n",
        "    print(\"2. Check that PROJECT_ROOT points to the correct directory\")\n",
        "    print(\"3. Verify 'llm_model' folder exists in Google Drive\")\n",
        "    print()\n",
        "    print(\"You can manually add the path with:\")\n",
        "    print(f\"   import sys\")\n",
        "    print(f\"   sys.path.insert(0, '{project_path}')\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第六步：初始化 STAC 分析器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建配置\n",
        "hf_config = HuggingFaceConfig(\n",
        "    model=HF_MODEL,\n",
        "    device=HF_DEVICE,\n",
        "    temperature=0.2,\n",
        "    top_p=0.9,\n",
        "    max_new_tokens=2048,\n",
        ")\n",
        "\n",
        "# 检查 LLMConfig 是否支持 huggingface 参数\n",
        "try:\n",
        "    # 检查 LLMConfig 是否有 huggingface 字段（检查 dataclass 字段）\n",
        "    has_huggingface = hasattr(LLMConfig, '__dataclass_fields__') and 'huggingface' in LLMConfig.__dataclass_fields__\n",
        "    \n",
        "    if has_huggingface:\n",
        "        llm_config = LLMConfig(\n",
        "            provider=\"huggingface\",\n",
        "            huggingface=hf_config,\n",
        "        )\n",
        "        print(\"✓ LLMConfig created with huggingface support\")\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            \"LLMConfig does not support 'huggingface' parameter. \"\n",
        "            \"Please update your llm_model/llm_router.py file in Google Drive. \"\n",
        "            \"The file should have 'huggingface: HuggingFaceConfig = HuggingFaceConfig()' in the LLMConfig class.\"\n",
        "        )\n",
        "        \n",
        "except TypeError as e:\n",
        "    print(f\"✗ {e}\")\n",
        "    print()\n",
        "    print(\"Solution:\")\n",
        "    print(\"1. Open llm_model/llm_router.py in your Google Drive\")\n",
        "    print(\"2. Make sure the LLMConfig class includes:\")\n",
        "    print(\"   huggingface: HuggingFaceConfig = HuggingFaceConfig()\")\n",
        "    print(\"3. Re-upload the file to Google Drive if needed\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error creating LLMConfig: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "stac_config = STACAnalyzerConfig(llm=llm_config)\n",
        "\n",
        "print(\"✓ STAC analyzer configured\")\n",
        "print(\"  Note: Model will be downloaded on first use (this may take a few minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 使用方式\n",
        "\n",
        "### 方式一：分析单个句子（测试用）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例：分析单个句子\n",
        "test_sentence = \"王子来到了森林里。\"\n",
        "story_context = \"从前有一个王子，他非常勇敢。王子来到了森林里。他在那里遇到了一个仙女。\"\n",
        "\n",
        "result = analyze_stac(\n",
        "    sentence=test_sentence,\n",
        "    story_context=story_context if USE_CONTEXT else None,\n",
        "    use_context=USE_CONTEXT,\n",
        "    config=stac_config,\n",
        ")\n",
        "\n",
        "print(\"Sentence:\", test_sentence)\n",
        "print(\"Result:\")\n",
        "import json\n",
        "print(json.dumps(result, ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 方式二：分析整个故事文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 配置输入输出路径 ==========\n",
        "# 修改为你的故事文件路径和输出路径\n",
        "STORY_FILE = \"/content/drive/MyDrive/path/to/your/story.txt\"\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/path/to/output/story_stac.json\"\n",
        "\n",
        "# 读取故事文件\n",
        "story_content = Path(STORY_FILE).read_text(encoding=\"utf-8\")\n",
        "print(f\"✓ Loaded story file: {STORY_FILE}\")\n",
        "print(f\"  Length: {len(story_content)} characters\")\n",
        "\n",
        "# 分割句子\n",
        "sentences = split_sentences_advanced(story_content)\n",
        "print(f\"✓ Split into {len(sentences)} sentences\")\n",
        "\n",
        "# 分析每个句子\n",
        "results = []\n",
        "for idx, sentence in enumerate(sentences, start=1):\n",
        "    print(f\"Processing sentence {idx}/{len(sentences)}: {sentence[:50]}...\")\n",
        "    \n",
        "    try:\n",
        "        # 获取邻近句子（如果启用）\n",
        "        previous_sentence = None\n",
        "        next_sentence = None\n",
        "        if USE_NEIGHBORING_SENTENCES:\n",
        "            sent_idx = idx - 1\n",
        "            if sent_idx > 0:\n",
        "                previous_sentence = sentences[sent_idx - 1]\n",
        "            if sent_idx < len(sentences) - 1:\n",
        "                next_sentence = sentences[sent_idx + 1]\n",
        "        \n",
        "        # 执行 STAC 分析\n",
        "        analysis = analyze_stac(\n",
        "            sentence=sentence,\n",
        "            story_context=story_content if USE_CONTEXT else None,\n",
        "            use_context=USE_CONTEXT,\n",
        "            previous_sentence=previous_sentence,\n",
        "            next_sentence=next_sentence,\n",
        "            use_neighboring_sentences=USE_NEIGHBORING_SENTENCES,\n",
        "            config=stac_config,\n",
        "        )\n",
        "        \n",
        "        results.append({\n",
        "            \"sentence_index\": idx,\n",
        "            \"sentence\": sentence,\n",
        "            \"analysis\": analysis,\n",
        "        })\n",
        "        \n",
        "        # 每10个句子显示进度\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"  ✓ Analyzed {idx}/{len(sentences)} sentences\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error analyzing sentence {idx}: {e}\")\n",
        "        results.append({\n",
        "            \"sentence_index\": idx,\n",
        "            \"sentence\": sentence,\n",
        "            \"analysis\": None,\n",
        "            \"error\": str(e),\n",
        "        })\n",
        "\n",
        "# 保存结果\n",
        "output_data = {\n",
        "    \"source_file\": STORY_FILE,\n",
        "    \"use_context\": USE_CONTEXT,\n",
        "    \"use_neighboring_sentences\": USE_NEIGHBORING_SENTENCES,\n",
        "    \"model\": HF_MODEL,\n",
        "    \"total_sentences\": len(sentences),\n",
        "    \"analyzed_sentences\": len(results),\n",
        "    \"sentences\": results,\n",
        "}\n",
        "\n",
        "Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
        "Path(OUTPUT_FILE).write_text(\n",
        "    json.dumps(output_data, ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Analysis complete!\")\n",
        "print(f\"  Output saved to: {OUTPUT_FILE}\")\n",
        "print(f\"  Analyzed: {len(results)}/{len(sentences)} sentences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 方式三：批量处理多个文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 批量处理配置 ==========\n",
        "INPUT_DIR = \"/content/drive/MyDrive/path/to/story_files\"  # 输入文件夹（包含 .txt 文件）\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/path/to/stac_output\"  # 输出文件夹\n",
        "\n",
        "# 查找所有文本文件\n",
        "input_path = Path(INPUT_DIR)\n",
        "output_path = Path(OUTPUT_DIR)\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "story_files = list(input_path.glob(\"*.txt\"))\n",
        "print(f\"Found {len(story_files)} story files\")\n",
        "\n",
        "# 批量处理\n",
        "for file_idx, story_file in enumerate(story_files, 1):\n",
        "    print(f\"\\n[{file_idx}/{len(story_files)}] Processing: {story_file.name}\")\n",
        "    \n",
        "    try:\n",
        "        # 读取文件\n",
        "        story_content = story_file.read_text(encoding=\"utf-8\")\n",
        "        sentences = split_sentences_advanced(story_content)\n",
        "        print(f\"  Split into {len(sentences)} sentences\")\n",
        "        \n",
        "        # 分析每个句子\n",
        "        results = []\n",
        "        for idx, sentence in enumerate(sentences, start=1):\n",
        "            try:\n",
        "                # 获取邻近句子\n",
        "                previous_sentence = None\n",
        "                next_sentence = None\n",
        "                if USE_NEIGHBORING_SENTENCES:\n",
        "                    sent_idx = idx - 1\n",
        "                    if sent_idx > 0:\n",
        "                        previous_sentence = sentences[sent_idx - 1]\n",
        "                    if sent_idx < len(sentences) - 1:\n",
        "                        next_sentence = sentences[sent_idx + 1]\n",
        "                \n",
        "                # STAC 分析\n",
        "                analysis = analyze_stac(\n",
        "                    sentence=sentence,\n",
        "                    story_context=story_content if USE_CONTEXT else None,\n",
        "                    use_context=USE_CONTEXT,\n",
        "                    previous_sentence=previous_sentence,\n",
        "                    next_sentence=next_sentence,\n",
        "                    use_neighboring_sentences=USE_NEIGHBORING_SENTENCES,\n",
        "                    config=stac_config,\n",
        "                )\n",
        "                \n",
        "                results.append({\n",
        "                    \"sentence_index\": idx,\n",
        "                    \"sentence\": sentence,\n",
        "                    \"analysis\": analysis,\n",
        "                })\n",
        "                \n",
        "                # 进度显示\n",
        "                if idx % 10 == 0:\n",
        "                    print(f\"    Analyzed {idx}/{len(sentences)} sentences...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"    ✗ Error at sentence {idx}: {e}\")\n",
        "                results.append({\n",
        "                    \"sentence_index\": idx,\n",
        "                    \"sentence\": sentence,\n",
        "                    \"analysis\": None,\n",
        "                    \"error\": str(e),\n",
        "                })\n",
        "        \n",
        "        # 保存结果\n",
        "        output_file = output_path / f\"{story_file.stem}_stac.json\"\n",
        "        output_data = {\n",
        "            \"source_file\": str(story_file),\n",
        "            \"use_context\": USE_CONTEXT,\n",
        "            \"use_neighboring_sentences\": USE_NEIGHBORING_SENTENCES,\n",
        "            \"model\": HF_MODEL,\n",
        "            \"total_sentences\": len(sentences),\n",
        "            \"analyzed_sentences\": len(results),\n",
        "            \"sentences\": results,\n",
        "        }\n",
        "        \n",
        "        output_file.write_text(\n",
        "            json.dumps(output_data, ensure_ascii=False, indent=2),\n",
        "            encoding=\"utf-8\"\n",
        "        )\n",
        "        \n",
        "        print(f\"  ✓ Saved to: {output_file.name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error processing {story_file.name}: {e}\")\n",
        "\n",
        "print(f\"\\n✓ Batch processing complete!\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 注意事项\n",
        "\n",
        "1. **内存限制**：Colab 免费版内存有限，建议使用较小的模型（如 `Qwen2.5-3B-Instruct`）\n",
        "2. **首次运行**：模型首次加载时需要从 Hugging Face 下载，可能需要几分钟\n",
        "3. **GPU 使用**：如果有 GPU，会自动使用 CUDA 加速\n",
        "4. **处理时间**：每个句子的分析需要几秒钟，大批量处理需要较长时间\n",
        "5. **中断恢复**：如果中断，可以修改代码跳过已处理的文件继续处理"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
