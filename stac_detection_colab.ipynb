{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STAC Detection on Google Colab\n",
        "\n",
        "è¿™ä¸ª notebook ç”¨äºåœ¨ Colab ä¸Šè¿è¡Œ STAC (Situation, Task, Action, Consequence) æ£€æµ‹ã€‚\n",
        "\n",
        "## ä½¿ç”¨è¯´æ˜\n",
        "\n",
        "1. **æŒ‚è½½ Google Drive**ï¼šç¡®ä¿ä½ çš„ `llm_model` å’Œ `pre_data_process` æ–‡ä»¶å¤¹å·²ç»åœ¨ Google Drive ä¸­ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼‰\n",
        "2. **é€‰æ‹©æ¨¡å‹**ï¼šæ”¯æŒ Hugging Face æ¨¡å‹ï¼ˆå¦‚ Qwen3ï¼‰\n",
        "3. **é…ç½®å‚æ•°**ï¼šè®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„å’Œæ¨¡å‹å‚æ•°\n",
        "4. **è¿è¡Œæ£€æµ‹**ï¼šå¯¹å•ä¸ªæ–‡ä»¶æˆ–æ‰¹é‡æ–‡ä»¶è¿›è¡Œ STAC æ£€æµ‹\n",
        "\n",
        "## æ”¯æŒçš„æ¨¡å‹\n",
        "\n",
        "- `Qwen/Qwen3-8B` ï¼ˆæ¨èï¼Œéœ€è¦è¾ƒå¤§å†…å­˜ï¼‰\n",
        "- å…¶ä»– Hugging Face èŠå¤©æ¨¡å‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬ä¸€æ­¥ï¼šæŒ‚è½½ Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŒ‚è½½ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ“ Google Drive mounted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬äºŒæ­¥ï¼šå®‰è£…ä¾èµ–åŒ…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…\n",
        "%pip install -q transformers torch accelerate sentencepiece\n",
        "\n",
        "# å¯é€‰ï¼šå®‰è£… vLLM ä»¥åŠ é€Ÿæ¨ç†ï¼ˆæ¨èåœ¨ A100 ç­‰é«˜ç«¯ GPU ä¸Šä½¿ç”¨ï¼‰\n",
        "# å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šä»¥å®‰è£… vLLM\n",
        "# %pip install -q vllm\n",
        "\n",
        "print(\"âœ“ Dependencies installed\")\n",
        "print(\"ğŸ’¡ Tip: For faster inference on A100, install vLLM by uncommenting the line above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬ä¸‰æ­¥ï¼šé…ç½®è·¯å¾„å’Œæ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ========== é…ç½®è·¯å¾„ ==========\n",
        "# ä¿®æ”¹ä¸ºä½  Google Drive ä¸­é¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„\n",
        "# ä¾‹å¦‚ï¼š/content/drive/MyDrive/fairytales_resarch\n",
        "# é¡¹ç›®ç»“æ„åº”è¯¥æ˜¯ï¼š\n",
        "#   fairytales_resarch/\n",
        "#     llm_model/\n",
        "#     pre_data_process/\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/fairytales_resarch\"\n",
        "\n",
        "# éªŒè¯è·¯å¾„\n",
        "project_path = Path(PROJECT_ROOT)\n",
        "llm_model_path = project_path / \"llm_model\"\n",
        "pre_data_process_path = project_path / \"pre_data_process\"\n",
        "\n",
        "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„ï¼ˆè¿™æ · llm_model å’Œ pre_data_process éƒ½èƒ½è¢«å¯¼å…¥ï¼‰\n",
        "if project_path.exists():\n",
        "    sys.path.insert(0, str(project_path))\n",
        "    print(f\"âœ“ Added project root to path: {project_path}\")\n",
        "    \n",
        "    # éªŒè¯å­ç›®å½•æ˜¯å¦å­˜åœ¨\n",
        "    if llm_model_path.exists():\n",
        "        print(f\"âœ“ Found llm_model: {llm_model_path}\")\n",
        "    else:\n",
        "        print(f\"âš  Warning: {llm_model_path} not found\")\n",
        "    \n",
        "    if pre_data_process_path.exists():\n",
        "        print(f\"âœ“ Found pre_data_process: {pre_data_process_path}\")\n",
        "    else:\n",
        "        print(f\"âš  Warning: {pre_data_process_path} not found\")\n",
        "else:\n",
        "    print(f\"âš  Warning: {PROJECT_ROOT} not found. Please check the path.\")\n",
        "\n",
        "# ========== é…ç½®æ¨¡å‹ ==========\n",
        "# é€‰æ‹© Hugging Face æ¨¡å‹\n",
        "# æ¨èä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ 3Bï¼‰å¦‚æœ Colab å†…å­˜ä¸è¶³\n",
        "HF_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"  # æˆ– \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "HF_DEVICE = \"auto\"  # \"cuda\", \"cpu\", æˆ– \"auto\"ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰\n",
        "\n",
        "# ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†ï¼ˆæ¨èåœ¨ A100 ç­‰é«˜ç«¯ GPU ä¸Šä½¿ç”¨ï¼‰\n",
        "# éœ€è¦å…ˆå®‰è£… vLLM: pip install vllm\n",
        "USE_VLLM = False  # è®¾ç½®ä¸º True ä»¥ä½¿ç”¨ vLLMï¼ˆæ›´å¿«ä½†éœ€è¦é¢å¤–å®‰è£…ï¼‰\n",
        "\n",
        "# ========== STAC åˆ†æé…ç½® ==========\n",
        "USE_CONTEXT = True  # ä½¿ç”¨å®Œæ•´æ•…äº‹ä¸Šä¸‹æ–‡\n",
        "USE_NEIGHBORING_SENTENCES = False  # ä½¿ç”¨ç›¸é‚»å¥å­ä½œä¸ºè¾…åŠ©ä¸Šä¸‹æ–‡\n",
        "\n",
        "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
        "os.environ[\"LLM_PROVIDER\"] = \"huggingface\"\n",
        "os.environ[\"HF_MODEL\"] = HF_MODEL\n",
        "os.environ[\"HF_DEVICE\"] = HF_DEVICE\n",
        "os.environ[\"HF_TEMPERATURE\"] = \"0.2\"\n",
        "os.environ[\"HF_TOP_P\"] = \"0.9\"\n",
        "os.environ[\"HF_MAX_NEW_TOKENS\"] = \"2048\"\n",
        "\n",
        "print(f\"âœ“ Configuration set\")\n",
        "print(f\"  Model: {HF_MODEL}\")\n",
        "print(f\"  Device: {HF_DEVICE}\")\n",
        "print(f\"  Use vLLM: {USE_VLLM} {'(faster, requires vLLM installed)' if USE_VLLM else '(set True + install vLLM for speedup on A100)'}\")\n",
        "print(f\"  Use Context: {USE_CONTEXT}\")\n",
        "print(f\"  Use Neighboring Sentences: {USE_NEIGHBORING_SENTENCES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬å››æ­¥ï¼šå®šä¹‰å¥å­åˆ†å‰²å‡½æ•°ï¼ˆå†…åµŒä»£ç ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¥å­åˆ†å‰²å‡½æ•°ï¼ˆä» pre_data_process å¤åˆ¶ï¼Œé¿å…å¯¼å…¥é—®é¢˜ï¼‰\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    \"\"\"å°†æ–‡æœ¬æŒ‰å¥å­åˆ‡åˆ†ï¼Œæ­£ç¡®å¤„ç†å¼•å·å†…çš„å¯¹è¯\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return []\n",
        "    \n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    \n",
        "    sentences = []\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "    \n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "        para = re.sub(r'\\n+', ' ', para)\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "        para_sentences = split_sentences_with_quotes(para)\n",
        "        sentences.extend(para_sentences)\n",
        "    \n",
        "    cleaned_sentences = []\n",
        "    for sent in sentences:\n",
        "        sent = sent.strip()\n",
        "        sent = re.sub(r'^[\\s\\u3000]+|[\\s\\u3000]+$', '', sent)\n",
        "        if sent and len(sent) > 0:\n",
        "            cleaned_sentences.append(sent)\n",
        "    \n",
        "    return cleaned_sentences\n",
        "\n",
        "\n",
        "def split_sentences_with_quotes(text: str) -> List[str]:\n",
        "    \"\"\"æ™ºèƒ½å¥å­åˆ‡åˆ†ï¼Œæ­£ç¡®å¤„ç†å¼•å·å†…çš„å¯¹è¯å’Œç ´æŠ˜å·\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    \n",
        "    double_quotes = {'\\u0022', '\\u201C', '\\u201D'}\n",
        "    single_quotes = {'\\u0027', '\\u2018', '\\u2019'}\n",
        "    chinese_quote_pairs = {'ã€Œ': 'ã€', 'ã€': 'ã€'}\n",
        "    \n",
        "    sentences = []\n",
        "    i = 0\n",
        "    current_sentence = []\n",
        "    quote_stack = []\n",
        "    sentence_end_chars = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ØŸ']\n",
        "    closing_chars = ['ã€', 'ã€', '\"', \"'\", '\"', \"'\", ')', 'ï¼‰']\n",
        "    dash_chars = ['â€”', 'â€“']\n",
        "    dash_pattern = 'â€”â€”'\n",
        "    \n",
        "    while i < len(text):\n",
        "        char = text[i]\n",
        "        \n",
        "        if char in chinese_quote_pairs:\n",
        "            quote_stack.append(char)\n",
        "            current_sentence.append(char)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        if char in chinese_quote_pairs.values():\n",
        "            if quote_stack and quote_stack[-1] in chinese_quote_pairs:\n",
        "                expected_close = chinese_quote_pairs[quote_stack[-1]]\n",
        "                if char == expected_close:\n",
        "                    quote_stack.pop()\n",
        "                    current_sentence.append(char)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        if char in double_quotes:\n",
        "            if quote_stack and quote_stack[-1] == 'double':\n",
        "                quote_stack.pop()\n",
        "                current_sentence.append(char)\n",
        "            else:\n",
        "                quote_stack.append('double')\n",
        "                current_sentence.append(char)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        if char in single_quotes:\n",
        "            if quote_stack and quote_stack[-1] == 'single':\n",
        "                quote_stack.pop()\n",
        "                current_sentence.append(char)\n",
        "            else:\n",
        "                quote_stack.append('single')\n",
        "                current_sentence.append(char)\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        if char in sentence_end_chars:\n",
        "            current_sentence.append(char)\n",
        "            if not quote_stack:\n",
        "                j = i + 1\n",
        "                while j < len(text) and text[j] in closing_chars:\n",
        "                    current_sentence.append(text[j])\n",
        "                    j += 1\n",
        "                while j < len(text) and text[j] in ' \\t\\n':\n",
        "                    j += 1\n",
        "                \n",
        "                if j < len(text):\n",
        "                    sentence = ''.join(current_sentence).strip()\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                    current_sentence = []\n",
        "                    i = j\n",
        "                    continue\n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        current_sentence.append(char)\n",
        "        i += 1\n",
        "    \n",
        "    if current_sentence:\n",
        "        sentence = ''.join(current_sentence).strip()\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "    \n",
        "    return sentences\n",
        "\n",
        "\n",
        "def split_sentences_advanced(text: str) -> List[str]:\n",
        "    \"\"\"é«˜çº§å¥å­åˆ‡åˆ†æ–¹æ³•ï¼Œå¤„ç†æ•°å­—ã€ç½‘å€ã€ç¼©å†™ç­‰\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return []\n",
        "    \n",
        "    protected_map = {}\n",
        "    protected_counter = 0\n",
        "    \n",
        "    def protect(match):\n",
        "        nonlocal protected_counter\n",
        "        placeholder = f\"<PROTECTED_{protected_counter}>\"\n",
        "        protected_map[placeholder] = match.group(0)\n",
        "        protected_counter += 1\n",
        "        return placeholder\n",
        "    \n",
        "    text = re.sub(r'\\d+\\.\\d+', protect, text)\n",
        "    text = re.sub(r'https?://[^\\s]+', protect, text)\n",
        "    text = re.sub(r'www\\.[^\\s]+', protect, text)\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', protect, text)\n",
        "    \n",
        "    abbreviations = [\n",
        "        r'\\bMr\\.', r'\\bMrs\\.', r'\\bMs\\.', r'\\bDr\\.', r'\\bProf\\.',\n",
        "        r'\\bSr\\.', r'\\bJr\\.', r'\\bInc\\.', r'\\bLtd\\.', r'\\bCo\\.',\n",
        "        r'\\betc\\.', r'\\bi\\.e\\.', r'\\be\\.g\\.', r'\\bvs\\.', r'\\bU\\.S\\.',\n",
        "    ]\n",
        "    for abbr in abbreviations:\n",
        "        text = re.sub(abbr, protect, text, flags=re.IGNORECASE)\n",
        "    \n",
        "    sentences = split_sentences(text)\n",
        "    \n",
        "    restored_sentences = []\n",
        "    for sent in sentences:\n",
        "        for placeholder, original in protected_map.items():\n",
        "            sent = sent.replace(placeholder, original)\n",
        "        restored_sentences.append(sent)\n",
        "    \n",
        "    return restored_sentences\n",
        "\n",
        "print(\"âœ“ Sentence splitting functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬äº”æ­¥ï¼šå¯¼å…¥ STAC åˆ†ææ¨¡å—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„æ¨¡å—ï¼ˆåªéœ€è¦ llm_modelï¼Œå¥å­åˆ†å‰²å‡½æ•°å·²åœ¨ä¸Šä¸€æ­¥å®šä¹‰ï¼‰\n",
        "try:\n",
        "    from llm_model.llm_router import LLMConfig\n",
        "    from llm_model.huggingface_client import HuggingFaceConfig\n",
        "    from llm_model.stac_analyzer import STACAnalyzerConfig, analyze_stac\n",
        "    \n",
        "    print(\"âœ“ All modules imported successfully\")\n",
        "    print(\"  - LLMConfig, HuggingFaceConfig\")\n",
        "    print(\"  - STACAnalyzerConfig, analyze_stac\")\n",
        "    print(\"  - split_sentences_advanced (defined in previous cell)\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"âœ— Import error: {e}\")\n",
        "    print()\n",
        "    print(\"Troubleshooting:\")\n",
        "    print(\"1. Make sure you ran the 'Configuration' cell (step 3) before this one\")\n",
        "    print(\"2. Check that PROJECT_ROOT points to the correct directory\")\n",
        "    print(\"3. Verify 'llm_model' folder exists in Google Drive\")\n",
        "    print()\n",
        "    print(\"You can manually add the path with:\")\n",
        "    print(f\"   import sys\")\n",
        "    print(f\"   sys.path.insert(0, '{project_path}')\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬å…­æ­¥ï¼šåˆå§‹åŒ– STAC åˆ†æå™¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºé…ç½®\n",
        "hf_config = HuggingFaceConfig(\n",
        "    model=HF_MODEL,\n",
        "    device=HF_DEVICE,\n",
        "    temperature=0.2,\n",
        "    top_p=0.9,\n",
        "    max_new_tokens=2048,\n",
        "    use_vllm=USE_VLLM,  # Use vLLM for faster inference on A100\n",
        ")\n",
        "\n",
        "# æ£€æŸ¥ LLMConfig æ˜¯å¦æ”¯æŒ huggingface å‚æ•°\n",
        "try:\n",
        "    # æ£€æŸ¥ LLMConfig æ˜¯å¦æœ‰ huggingface å­—æ®µï¼ˆæ£€æŸ¥ dataclass å­—æ®µï¼‰\n",
        "    has_huggingface = hasattr(LLMConfig, '__dataclass_fields__') and 'huggingface' in LLMConfig.__dataclass_fields__\n",
        "    \n",
        "    if has_huggingface:\n",
        "        llm_config = LLMConfig(\n",
        "            provider=\"huggingface\",\n",
        "            huggingface=hf_config,\n",
        "        )\n",
        "        print(\"âœ“ LLMConfig created with huggingface support\")\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            \"LLMConfig does not support 'huggingface' parameter. \"\n",
        "            \"Please update your llm_model/llm_router.py file in Google Drive. \"\n",
        "            \"The file should have 'huggingface: HuggingFaceConfig = HuggingFaceConfig()' in the LLMConfig class.\"\n",
        "        )\n",
        "        \n",
        "except TypeError as e:\n",
        "    print(f\"âœ— {e}\")\n",
        "    print()\n",
        "    print(\"Solution:\")\n",
        "    print(\"1. Open llm_model/llm_router.py in your Google Drive\")\n",
        "    print(\"2. Make sure the LLMConfig class includes:\")\n",
        "    print(\"   huggingface: HuggingFaceConfig = HuggingFaceConfig()\")\n",
        "    print(\"3. Re-upload the file to Google Drive if needed\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"âœ— Error creating LLMConfig: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "stac_config = STACAnalyzerConfig(llm=llm_config)\n",
        "\n",
        "print(\"âœ“ STAC analyzer configured\")\n",
        "print(\"  Note: Model will be downloaded on first use (this may take a few minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä½¿ç”¨æ–¹å¼\n",
        "\n",
        "### æ–¹å¼ä¸€ï¼šåˆ†æå•ä¸ªå¥å­ï¼ˆæµ‹è¯•ç”¨ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç¤ºä¾‹ï¼šåˆ†æå•ä¸ªå¥å­\n",
        "test_sentence = \"ç‹å­æ¥åˆ°äº†æ£®æ—é‡Œã€‚\"\n",
        "story_context = \"ä»å‰æœ‰ä¸€ä¸ªç‹å­ï¼Œä»–éå¸¸å‹‡æ•¢ã€‚ç‹å­æ¥åˆ°äº†æ£®æ—é‡Œã€‚ä»–åœ¨é‚£é‡Œé‡åˆ°äº†ä¸€ä¸ªä»™å¥³ã€‚\"\n",
        "\n",
        "result = analyze_stac(\n",
        "    sentence=test_sentence,\n",
        "    story_context=story_context if USE_CONTEXT else None,\n",
        "    use_context=USE_CONTEXT,\n",
        "    config=stac_config,\n",
        ")\n",
        "\n",
        "print(\"Sentence:\", test_sentence)\n",
        "print(\"Result:\")\n",
        "import json\n",
        "print(json.dumps(result, ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### æ–¹å¼äºŒï¼šåˆ†ææ•´ä¸ªæ•…äº‹æ–‡ä»¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== é…ç½®è¾“å…¥è¾“å‡ºè·¯å¾„ ==========\n",
        "# ä¿®æ”¹ä¸ºä½ çš„æ•…äº‹æ–‡ä»¶è·¯å¾„å’Œè¾“å‡ºè·¯å¾„\n",
        "STORY_FILE = \"/content/drive/MyDrive/path/to/your/story.txt\"\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/path/to/output/story_stac.json\"\n",
        "\n",
        "# è¯»å–æ•…äº‹æ–‡ä»¶\n",
        "story_content = Path(STORY_FILE).read_text(encoding=\"utf-8\")\n",
        "print(f\"âœ“ Loaded story file: {STORY_FILE}\")\n",
        "print(f\"  Length: {len(story_content)} characters\")\n",
        "\n",
        "# åˆ†å‰²å¥å­\n",
        "sentences = split_sentences_advanced(story_content)\n",
        "print(f\"âœ“ Split into {len(sentences)} sentences\")\n",
        "\n",
        "# åˆ†ææ¯ä¸ªå¥å­\n",
        "results = []\n",
        "for idx, sentence in enumerate(sentences, start=1):\n",
        "    print(f\"Processing sentence {idx}/{len(sentences)}: {sentence[:50]}...\")\n",
        "    \n",
        "    try:\n",
        "        # è·å–é‚»è¿‘å¥å­ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
        "        previous_sentence = None\n",
        "        next_sentence = None\n",
        "        if USE_NEIGHBORING_SENTENCES:\n",
        "            sent_idx = idx - 1\n",
        "            if sent_idx > 0:\n",
        "                previous_sentence = sentences[sent_idx - 1]\n",
        "            if sent_idx < len(sentences) - 1:\n",
        "                next_sentence = sentences[sent_idx + 1]\n",
        "        \n",
        "        # æ‰§è¡Œ STAC åˆ†æ\n",
        "        analysis = analyze_stac(\n",
        "            sentence=sentence,\n",
        "            story_context=story_content if USE_CONTEXT else None,\n",
        "            use_context=USE_CONTEXT,\n",
        "            previous_sentence=previous_sentence,\n",
        "            next_sentence=next_sentence,\n",
        "            use_neighboring_sentences=USE_NEIGHBORING_SENTENCES,\n",
        "            config=stac_config,\n",
        "        )\n",
        "        \n",
        "        results.append({\n",
        "            \"sentence_index\": idx,\n",
        "            \"sentence\": sentence,\n",
        "            \"analysis\": analysis,\n",
        "        })\n",
        "        \n",
        "        # æ¯10ä¸ªå¥å­æ˜¾ç¤ºè¿›åº¦\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"  âœ“ Analyzed {idx}/{len(sentences)} sentences\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error analyzing sentence {idx}: {e}\")\n",
        "        results.append({\n",
        "            \"sentence_index\": idx,\n",
        "            \"sentence\": sentence,\n",
        "            \"analysis\": None,\n",
        "            \"error\": str(e),\n",
        "        })\n",
        "\n",
        "# ä¿å­˜ç»“æœ\n",
        "output_data = {\n",
        "    \"source_file\": STORY_FILE,\n",
        "    \"use_context\": USE_CONTEXT,\n",
        "    \"use_neighboring_sentences\": USE_NEIGHBORING_SENTENCES,\n",
        "    \"model\": HF_MODEL,\n",
        "    \"total_sentences\": len(sentences),\n",
        "    \"analyzed_sentences\": len(results),\n",
        "    \"sentences\": results,\n",
        "}\n",
        "\n",
        "Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
        "Path(OUTPUT_FILE).write_text(\n",
        "    json.dumps(output_data, ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Analysis complete!\")\n",
        "print(f\"  Output saved to: {OUTPUT_FILE}\")\n",
        "print(f\"  Analyzed: {len(results)}/{len(sentences)} sentences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### æ–¹å¼ä¸‰ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== æ‰¹é‡å¤„ç†é…ç½® ==========\n",
        "INPUT_DIR = \"/content/drive/MyDrive/path/to/story_files\"  # è¾“å…¥æ–‡ä»¶å¤¹ï¼ˆåŒ…å« .txt æ–‡ä»¶ï¼‰\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/path/to/stac_output\"  # è¾“å‡ºæ–‡ä»¶å¤¹\n",
        "\n",
        "# æŸ¥æ‰¾æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶\n",
        "input_path = Path(INPUT_DIR)\n",
        "output_path = Path(OUTPUT_DIR)\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "story_files = list(input_path.glob(\"*.txt\"))\n",
        "print(f\"Found {len(story_files)} story files\")\n",
        "\n",
        "# æ‰¹é‡å¤„ç†\n",
        "for file_idx, story_file in enumerate(story_files, 1):\n",
        "    print(f\"\\n[{file_idx}/{len(story_files)}] Processing: {story_file.name}\")\n",
        "    \n",
        "    try:\n",
        "        # è¯»å–æ–‡ä»¶\n",
        "        story_content = story_file.read_text(encoding=\"utf-8\")\n",
        "        sentences = split_sentences_advanced(story_content)\n",
        "        print(f\"  Split into {len(sentences)} sentences\")\n",
        "        \n",
        "        # åˆ†ææ¯ä¸ªå¥å­\n",
        "        results = []\n",
        "        for idx, sentence in enumerate(sentences, start=1):\n",
        "            try:\n",
        "                # è·å–é‚»è¿‘å¥å­\n",
        "                previous_sentence = None\n",
        "                next_sentence = None\n",
        "                if USE_NEIGHBORING_SENTENCES:\n",
        "                    sent_idx = idx - 1\n",
        "                    if sent_idx > 0:\n",
        "                        previous_sentence = sentences[sent_idx - 1]\n",
        "                    if sent_idx < len(sentences) - 1:\n",
        "                        next_sentence = sentences[sent_idx + 1]\n",
        "                \n",
        "                # STAC åˆ†æ\n",
        "                analysis = analyze_stac(\n",
        "                    sentence=sentence,\n",
        "                    story_context=story_content if USE_CONTEXT else None,\n",
        "                    use_context=USE_CONTEXT,\n",
        "                    previous_sentence=previous_sentence,\n",
        "                    next_sentence=next_sentence,\n",
        "                    use_neighboring_sentences=USE_NEIGHBORING_SENTENCES,\n",
        "                    config=stac_config,\n",
        "                )\n",
        "                \n",
        "                results.append({\n",
        "                    \"sentence_index\": idx,\n",
        "                    \"sentence\": sentence,\n",
        "                    \"analysis\": analysis,\n",
        "                })\n",
        "                \n",
        "                # è¿›åº¦æ˜¾ç¤º\n",
        "                if idx % 10 == 0:\n",
        "                    print(f\"    Analyzed {idx}/{len(sentences)} sentences...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"    âœ— Error at sentence {idx}: {e}\")\n",
        "                results.append({\n",
        "                    \"sentence_index\": idx,\n",
        "                    \"sentence\": sentence,\n",
        "                    \"analysis\": None,\n",
        "                    \"error\": str(e),\n",
        "                })\n",
        "        \n",
        "        # ä¿å­˜ç»“æœ\n",
        "        output_file = output_path / f\"{story_file.stem}_stac.json\"\n",
        "        output_data = {\n",
        "            \"source_file\": str(story_file),\n",
        "            \"use_context\": USE_CONTEXT,\n",
        "            \"use_neighboring_sentences\": USE_NEIGHBORING_SENTENCES,\n",
        "            \"model\": HF_MODEL,\n",
        "            \"total_sentences\": len(sentences),\n",
        "            \"analyzed_sentences\": len(results),\n",
        "            \"sentences\": results,\n",
        "        }\n",
        "        \n",
        "        output_file.write_text(\n",
        "            json.dumps(output_data, ensure_ascii=False, indent=2),\n",
        "            encoding=\"utf-8\"\n",
        "        )\n",
        "        \n",
        "        print(f\"  âœ“ Saved to: {output_file.name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error processing {story_file.name}: {e}\")\n",
        "\n",
        "print(f\"\\nâœ“ Batch processing complete!\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ³¨æ„äº‹é¡¹\n",
        "\n",
        "1. **å†…å­˜é™åˆ¶**ï¼šColab å…è´¹ç‰ˆå†…å­˜æœ‰é™ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ `Qwen2.5-3B-Instruct`ï¼‰\n",
        "2. **é¦–æ¬¡è¿è¡Œ**ï¼šæ¨¡å‹é¦–æ¬¡åŠ è½½æ—¶éœ€è¦ä» Hugging Face ä¸‹è½½ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ\n",
        "3. **GPU ä½¿ç”¨**ï¼šå¦‚æœæœ‰ GPUï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨ CUDA åŠ é€Ÿ\n",
        "4. **å¤„ç†æ—¶é—´**ï¼šæ¯ä¸ªå¥å­çš„åˆ†æéœ€è¦å‡ ç§’é’Ÿï¼Œå¤§æ‰¹é‡å¤„ç†éœ€è¦è¾ƒé•¿æ—¶é—´\n",
        "5. **ä¸­æ–­æ¢å¤**ï¼šå¦‚æœä¸­æ–­ï¼Œå¯ä»¥ä¿®æ”¹ä»£ç è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ç»§ç»­å¤„ç†"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
