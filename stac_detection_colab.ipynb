{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STAC Detection on Google Colab\n",
        "\n",
        "这个 notebook 用于在 Colab 上运行 STAC (Situation, Task, Action, Consequence) 检测。\n",
        "\n",
        "## 使用说明\n",
        "\n",
        "1. **挂载 Google Drive**：确保你的 `llm_model` 和 `pre_data_process` 文件夹已经在 Google Drive 中（在项目根目录下）\n",
        "2. **选择模型**：支持 Hugging Face 模型（如 Qwen3）\n",
        "3. **配置参数**：设置输入输出路径和模型参数\n",
        "4. **运行检测**：对单个文件或批量文件进行 STAC 检测\n",
        "\n",
        "## 支持的模型\n",
        "\n",
        "- `Qwen/Qwen2.5-7B-Instruct` （推荐，需要较大内存）\n",
        "- `Qwen/Qwen2.5-3B-Instruct` （较小模型，适合 Colab 免费版）\n",
        "- `Qwen/Qwen2-7B-Instruct`\n",
        "- 其他 Hugging Face 聊天模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第一步：挂载 Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 挂载 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"✓ Google Drive mounted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第二步：安装依赖包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装必要的依赖包\n",
        "%pip install -q transformers torch accelerate sentencepiece\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第三步：配置路径和模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ========== 配置路径 ==========\n",
        "# 修改为你 Google Drive 中项目根目录的路径\n",
        "# 例如：/content/drive/MyDrive/fairytales_resarch\n",
        "# 项目结构应该是：\n",
        "#   fairytales_resarch/\n",
        "#     llm_model/\n",
        "#     pre_data_process/\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/fairytales_resarch\"\n",
        "\n",
        "# 验证路径\n",
        "project_path = Path(PROJECT_ROOT)\n",
        "llm_model_path = project_path / \"llm_model\"\n",
        "pre_data_process_path = project_path / \"pre_data_process\"\n",
        "\n",
        "# 添加项目根目录到 Python 路径（这样 llm_model 和 pre_data_process 都能被导入）\n",
        "if project_path.exists():\n",
        "    sys.path.insert(0, str(project_path))\n",
        "    print(f\"✓ Added project root to path: {project_path}\")\n",
        "    \n",
        "    # 验证子目录是否存在\n",
        "    if llm_model_path.exists():\n",
        "        print(f\"✓ Found llm_model: {llm_model_path}\")\n",
        "    else:\n",
        "        print(f\"⚠ Warning: {llm_model_path} not found\")\n",
        "    \n",
        "    if pre_data_process_path.exists():\n",
        "        print(f\"✓ Found pre_data_process: {pre_data_process_path}\")\n",
        "    else:\n",
        "        print(f\"⚠ Warning: {pre_data_process_path} not found\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: {PROJECT_ROOT} not found. Please check the path.\")\n",
        "\n",
        "# ========== 配置模型 ==========\n",
        "# 选择 Hugging Face 模型\n",
        "# 推荐使用较小的模型（如 3B）如果 Colab 内存不足\n",
        "HF_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"  # 或 \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "HF_DEVICE = \"auto\"  # \"cuda\", \"cpu\", 或 \"auto\"（自动检测）\n",
        "\n",
        "# ========== STAC 分析配置 ==========\n",
        "USE_CONTEXT = True  # 使用完整故事上下文\n",
        "USE_NEIGHBORING_SENTENCES = False  # 使用相邻句子作为辅助上下文\n",
        "\n",
        "# 设置环境变量\n",
        "os.environ[\"LLM_PROVIDER\"] = \"huggingface\"\n",
        "os.environ[\"HF_MODEL\"] = HF_MODEL\n",
        "os.environ[\"HF_DEVICE\"] = HF_DEVICE\n",
        "os.environ[\"HF_TEMPERATURE\"] = \"0.2\"\n",
        "os.environ[\"HF_TOP_P\"] = \"0.9\"\n",
        "os.environ[\"HF_MAX_NEW_TOKENS\"] = \"2048\"\n",
        "\n",
        "print(f\"✓ Configuration set\")\n",
        "print(f\"  Model: {HF_MODEL}\")\n",
        "print(f\"  Device: {HF_DEVICE}\")\n",
        "print(f\"  Use Context: {USE_CONTEXT}\")\n",
        "print(f\"  Use Neighboring Sentences: {USE_NEIGHBORING_SENTENCES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第四步：导入 STAC 分析模块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入必要的模块\n",
        "from llm_model.llm_router import LLMConfig\n",
        "from llm_model.huggingface_client import HuggingFaceConfig\n",
        "from llm_model.stac_analyzer import STACAnalyzerConfig, analyze_stac\n",
        "from pre_data_process.sentence_splitter import split_sentences_advanced\n",
        "\n",
        "print(\"✓ Modules imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 第五步：初始化 STAC 分析器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建配置\n",
        "hf_config = HuggingFaceConfig(\n",
        "    model=HF_MODEL,\n",
        "    device=HF_DEVICE,\n",
        "    temperature=0.2,\n",
        "    top_p=0.9,\n",
        "    max_new_tokens=2048,\n",
        ")\n",
        "\n",
        "llm_config = LLMConfig(\n",
        "    provider=\"huggingface\",\n",
        "    huggingface=hf_config,\n",
        ")\n",
        "\n",
        "stac_config = STACAnalyzerConfig(llm=llm_config)\n",
        "\n",
        "print(\"✓ STAC analyzer configured\")\n",
        "print(\"  Note: Model will be downloaded on first use (this may take a few minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 使用方式\n",
        "\n",
        "### 方式一：分析单个句子（测试用）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 示例：分析单个句子\n",
        "test_sentence = \"王子来到了森林里。\"\n",
        "story_context = \"从前有一个王子，他非常勇敢。王子来到了森林里。他在那里遇到了一个仙女。\"\n",
        "\n",
        "result = analyze_stac(\n",
        "    sentence=test_sentence,\n",
        "    story_context=story_context if USE_CONTEXT else None,\n",
        "    use_context=USE_CONTEXT,\n",
        "    config=stac_config,\n",
        ")\n",
        "\n",
        "print(\"Sentence:\", test_sentence)\n",
        "print(\"Result:\")\n",
        "import json\n",
        "print(json.dumps(result, ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 方式二：分析整个故事文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 配置输入输出路径 ==========\n",
        "# 修改为你的故事文件路径和输出路径\n",
        "STORY_FILE = \"/content/drive/MyDrive/path/to/your/story.txt\"\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/path/to/output/story_stac.json\"\n",
        "\n",
        "# 读取故事文件\n",
        "story_content = Path(STORY_FILE).read_text(encoding=\"utf-8\")\n",
        "print(f\"✓ Loaded story file: {STORY_FILE}\")\n",
        "print(f\"  Length: {len(story_content)} characters\")\n",
        "\n",
        "# 分割句子\n",
        "sentences = split_sentences_advanced(story_content)\n",
        "print(f\"✓ Split into {len(sentences)} sentences\")\n",
        "\n",
        "# 分析每个句子\n",
        "results = []\n",
        "for idx, sentence in enumerate(sentences, start=1):\n",
        "    print(f\"Processing sentence {idx}/{len(sentences)}: {sentence[:50]}...\")\n",
        "    \n",
        "    try:\n",
        "        # 获取邻近句子（如果启用）\n",
        "        previous_sentence = None\n",
        "        next_sentence = None\n",
        "        if USE_NEIGHBORING_SENTENCES:\n",
        "            sent_idx = idx - 1\n",
        "            if sent_idx > 0:\n",
        "                previous_sentence = sentences[sent_idx - 1]\n",
        "            if sent_idx < len(sentences) - 1:\n",
        "                next_sentence = sentences[sent_idx + 1]\n",
        "        \n",
        "        # 执行 STAC 分析\n",
        "        analysis = analyze_stac(\n",
        "            sentence=sentence,\n",
        "            story_context=story_content if USE_CONTEXT else None,\n",
        "            use_context=USE_CONTEXT,\n",
        "            previous_sentence=previous_sentence,\n",
        "            next_sentence=next_sentence,\n",
        "            use_neighboring_sentences=USE_NEIGHBORING_SENTENCES,\n",
        "            config=stac_config,\n",
        "        )\n",
        "        \n",
        "        results.append({\n",
        "            \"sentence_index\": idx,\n",
        "            \"sentence\": sentence,\n",
        "            \"analysis\": analysis,\n",
        "        })\n",
        "        \n",
        "        # 每10个句子显示进度\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"  ✓ Analyzed {idx}/{len(sentences)} sentences\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error analyzing sentence {idx}: {e}\")\n",
        "        results.append({\n",
        "            \"sentence_index\": idx,\n",
        "            \"sentence\": sentence,\n",
        "            \"analysis\": None,\n",
        "            \"error\": str(e),\n",
        "        })\n",
        "\n",
        "# 保存结果\n",
        "output_data = {\n",
        "    \"source_file\": STORY_FILE,\n",
        "    \"use_context\": USE_CONTEXT,\n",
        "    \"use_neighboring_sentences\": USE_NEIGHBORING_SENTENCES,\n",
        "    \"model\": HF_MODEL,\n",
        "    \"total_sentences\": len(sentences),\n",
        "    \"analyzed_sentences\": len(results),\n",
        "    \"sentences\": results,\n",
        "}\n",
        "\n",
        "Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
        "Path(OUTPUT_FILE).write_text(\n",
        "    json.dumps(output_data, ensure_ascii=False, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Analysis complete!\")\n",
        "print(f\"  Output saved to: {OUTPUT_FILE}\")\n",
        "print(f\"  Analyzed: {len(results)}/{len(sentences)} sentences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 方式三：批量处理多个文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 批量处理配置 ==========\n",
        "INPUT_DIR = \"/content/drive/MyDrive/path/to/story_files\"  # 输入文件夹（包含 .txt 文件）\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/path/to/stac_output\"  # 输出文件夹\n",
        "\n",
        "# 查找所有文本文件\n",
        "input_path = Path(INPUT_DIR)\n",
        "output_path = Path(OUTPUT_DIR)\n",
        "output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "story_files = list(input_path.glob(\"*.txt\"))\n",
        "print(f\"Found {len(story_files)} story files\")\n",
        "\n",
        "# 批量处理\n",
        "for file_idx, story_file in enumerate(story_files, 1):\n",
        "    print(f\"\\n[{file_idx}/{len(story_files)}] Processing: {story_file.name}\")\n",
        "    \n",
        "    try:\n",
        "        # 读取文件\n",
        "        story_content = story_file.read_text(encoding=\"utf-8\")\n",
        "        sentences = split_sentences_advanced(story_content)\n",
        "        print(f\"  Split into {len(sentences)} sentences\")\n",
        "        \n",
        "        # 分析每个句子\n",
        "        results = []\n",
        "        for idx, sentence in enumerate(sentences, start=1):\n",
        "            try:\n",
        "                # 获取邻近句子\n",
        "                previous_sentence = None\n",
        "                next_sentence = None\n",
        "                if USE_NEIGHBORING_SENTENCES:\n",
        "                    sent_idx = idx - 1\n",
        "                    if sent_idx > 0:\n",
        "                        previous_sentence = sentences[sent_idx - 1]\n",
        "                    if sent_idx < len(sentences) - 1:\n",
        "                        next_sentence = sentences[sent_idx + 1]\n",
        "                \n",
        "                # STAC 分析\n",
        "                analysis = analyze_stac(\n",
        "                    sentence=sentence,\n",
        "                    story_context=story_content if USE_CONTEXT else None,\n",
        "                    use_context=USE_CONTEXT,\n",
        "                    previous_sentence=previous_sentence,\n",
        "                    next_sentence=next_sentence,\n",
        "                    use_neighboring_sentences=USE_NEIGHBORING_SENTENCES,\n",
        "                    config=stac_config,\n",
        "                )\n",
        "                \n",
        "                results.append({\n",
        "                    \"sentence_index\": idx,\n",
        "                    \"sentence\": sentence,\n",
        "                    \"analysis\": analysis,\n",
        "                })\n",
        "                \n",
        "                # 进度显示\n",
        "                if idx % 10 == 0:\n",
        "                    print(f\"    Analyzed {idx}/{len(sentences)} sentences...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"    ✗ Error at sentence {idx}: {e}\")\n",
        "                results.append({\n",
        "                    \"sentence_index\": idx,\n",
        "                    \"sentence\": sentence,\n",
        "                    \"analysis\": None,\n",
        "                    \"error\": str(e),\n",
        "                })\n",
        "        \n",
        "        # 保存结果\n",
        "        output_file = output_path / f\"{story_file.stem}_stac.json\"\n",
        "        output_data = {\n",
        "            \"source_file\": str(story_file),\n",
        "            \"use_context\": USE_CONTEXT,\n",
        "            \"use_neighboring_sentences\": USE_NEIGHBORING_SENTENCES,\n",
        "            \"model\": HF_MODEL,\n",
        "            \"total_sentences\": len(sentences),\n",
        "            \"analyzed_sentences\": len(results),\n",
        "            \"sentences\": results,\n",
        "        }\n",
        "        \n",
        "        output_file.write_text(\n",
        "            json.dumps(output_data, ensure_ascii=False, indent=2),\n",
        "            encoding=\"utf-8\"\n",
        "        )\n",
        "        \n",
        "        print(f\"  ✓ Saved to: {output_file.name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error processing {story_file.name}: {e}\")\n",
        "\n",
        "print(f\"\\n✓ Batch processing complete!\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 注意事项\n",
        "\n",
        "1. **内存限制**：Colab 免费版内存有限，建议使用较小的模型（如 `Qwen2.5-3B-Instruct`）\n",
        "2. **首次运行**：模型首次加载时需要从 Hugging Face 下载，可能需要几分钟\n",
        "3. **GPU 使用**：如果有 GPU，会自动使用 CUDA 加速\n",
        "4. **处理时间**：每个句子的分析需要几秒钟，大批量处理需要较长时间\n",
        "5. **中断恢复**：如果中断，可以修改代码跳过已处理的文件继续处理"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
