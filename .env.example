# Copy to `.env` (DO NOT commit `.env`).

# Which LLM backend to use: ollama (local Qwen3) or gemini (cloud)
LLM_PROVIDER=ollama
# 1/true enables thinking mode (only meaningful for Gemini when GEMINI_MODEL_THINKING is set)
LLM_THINKING=0

# ---- Ollama (local) ----
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=qwen3:8b
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:4b

# ---- Gemini (cloud) ----
# Create an API key in Google AI Studio / Google Cloud and set it here.
GEMINI_API_KEY=

# Set these to the model names available in your account.
# Examples you might use (names can change over time):
# GEMINI_MODEL=gemini-2.0-flash
# GEMINI_MODEL_THINKING=gemini-2.0-flash-thinking
GEMINI_MODEL=gemini-3.0-flash
GEMINI_MODEL_THINKING=gemini-3.0-flash-thinking

# Optional generation tuning
GEMINI_TEMPERATURE=0.2
GEMINI_TOP_P=0.9
GEMINI_MAX_OUTPUT_TOKENS=8192
