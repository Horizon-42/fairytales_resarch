# Technical Design: Local Vector DB for ATU + Motif Detection

Goal: given a story text, quickly retrieve likely **ATU types** and **TMI motifs**.

This module builds a local, file-based vector database from:

- `docs/ATU_Resources/ATU_types_complete.csv`
- `docs/Motif/tmi_v.1.2.csv`

Embeddings are generated by **Ollama** using the embedding model **`qwen3-embedding:4b`**.

## 1. Architecture overview

This system is intentionally split into two layers:

1) **Metadata/text store (SQLite)**
- Stores each ATU type / motif as a document with stable integer `id`.
- Keeps original metadata needed for later display (e.g. `atu_number`, `code`, titles, categories).

2) **Vector index (HNSW via hnswlib)**
- Stores dense vectors and supports fast approximate nearest-neighbor (ANN) search.
- Built as two separate indices:
  - `atu_hnsw.bin`
  - `motif_hnsw.bin`

A small `meta.json` records:
- embedding model name
- embedding dimension (for `qwen3-embedding:4b`, it is typically 2560)
- HNSW parameters
- collection counts

### Why HNSW + SQLite (instead of a “full” vector DB)?

For a single-machine research workflow:

- `hnswlib` provides excellent latency/recall tradeoffs and is widely used.
- `sqlite3` is in the Python standard library and is reliable for metadata storage.
- No daemon/service is required; the DB is just files in `store/`.

This choice optimizes for:
- simplicity
- reproducibility
- speed on a single node

If you later need multi-user, distributed, or remote serving, a good next step is Qdrant.

## 2. Data model

Each row in the CSV becomes one “document”:

### ATU document

- **Key**: `atu:{atu_number}`
- **Text used for embedding**: a short, structured string:
  - `ATU {atu_number}: {title}`
  - category path (level1/2/3)
  - `description`
- **Metadata**: `atu_number`, `title`, categories, `detail_url`, etc.

### Motif document

- **Key**: `motif:{code}`
- **Text used for embedding**: `Motif {code}: {MOTIF}` plus limited hierarchy fields.
- **Metadata**: `code`, `motif`, chapter/divisions.

All documents are stored in SQLite table `documents`.

Important design constraint:
- We rely on SQLite’s integer primary key `id` as the **label id** stored in the HNSW index.
  That makes doc lookup O(1) by `id` and avoids maintaining a separate id-map file.

## 3. Build pipeline

Build entry point:

- `python -m llm_model.vector_database.cli build`

Steps:

1) **Parse CSVs** into `DocRecord`s (collection = `atu` or `motif`).
2) **Upsert** into SQLite (`docs.sqlite`).
3) **Determine embedding dimension** using a sample embedding call.
4) **Build HNSW index for ATU**
   - Iterate documents from SQLite.
   - Batch texts and call Ollama embedding endpoint.
   - Add vectors with labels equal to SQLite `id`.
5) **Build HNSW index for Motif** (same as ATU).
6) Write `meta.json`.

### Embedding calls (Ollama)

The embedding client prefers the batch endpoint:

- `POST /api/embed` with `{model, input: [..]}`

If the server doesn’t support batch, it falls back to:

- `POST /api/embeddings` with `{model, prompt}`

Batching matters because the motif file contains ~46k rows.

## 4. Query pipeline (“detect”)

Query entry point:

- `python -m llm_model.vector_database.cli detect --text-file ...`

Steps:

1) **Chunk the story text** using a simple character-based chunker.
   - Default: `max_chars=1200`, `overlap=120`
   - Rationale: long stories contain multiple motifs; chunking improves recall.

2) **Embed each chunk** using the same embedding model.

3) **KNN search** against each collection index:
   - For each chunk vector, search top-k neighbors.
   - Convert HNSW cosine distance to similarity:

$$\text{cosine\_sim} = 1 - \text{cosine\_distance}$$

4) **Aggregate matches across chunks**
   - For each document id, keep the **best (max) similarity** observed across all chunks.
   - This reduces duplicated results and gives a single score per ATU/motif.

5) **Thresholding**
   - Keep results with similarity >= configured threshold.
   - Defaults:
     - `atu_min_similarity = 0.45`
     - `motif_min_similarity = 0.35`

These are heuristic starting points; tune based on your story language and data.

## 5. Files and persistence

Output directory: `llm_model/vector_database/store/`

- `docs.sqlite`: documents + metadata
- `atu_hnsw.bin`: ATU HNSW index
- `motif_hnsw.bin`: Motif HNSW index
- `meta.json`: dimension + settings

Rebuilding from scratch is deterministic given the same CSVs, embedding model, and HNSW params.

## 6. Performance notes

- Motifs (~46k) dominate build time.
- Larger `--embed-batch-size` often speeds up building significantly (within Ollama memory limits).
- Query latency is usually dominated by embedding the story chunks.

## 7. Limitations

- This is **semantic retrieval**, not a guarantee of “contained motif”.
  It returns candidates that are semantically similar to chunks.
- The quality depends on how well the story text aligns with the English motif descriptions.
  If you primarily run Chinese stories, you may consider:
  - translating story into English before detection, or
  - building bilingual motif representations (future improvement).
