from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

from llm_model.ollama_client import embed as ollama_embed

from .csv_sources import SourcePaths, iter_atu_records, iter_motif_records
from .hnsw_index import HNSWConfig, HNSWIndex
from .paths import VectorDBPaths, default_paths
from .sqlite_store import DocRecord, connect, count_collection, ensure_schema, fetch_by_ids, upsert_documents
from .text_chunking import ChunkingConfig, chunk_text


def _cosine_distance_to_similarity(distance: float) -> float:
    # For hnswlib cosine space, distance is (1 - cosine_similarity).
    return 1.0 - float(distance)


@dataclass(frozen=True)
class BuildConfig:
    ollama_base_url: str = "http://localhost:11434"
    embedding_model: str = "qwen3-embedding:4b"

    # Batching controls
    embed_batch_size: int = 32

    # HNSW controls
    hnsw: HNSWConfig = HNSWConfig()


@dataclass(frozen=True)
class QueryConfig:
    ollama_base_url: str = "http://localhost:11434"
    embedding_model: str = "qwen3-embedding:4b"

    top_k: int = 10
    chunking: ChunkingConfig = ChunkingConfig()

    # Similarity thresholds (cosine similarity)
    atu_min_similarity: float = 0.45
    motif_min_similarity: float = 0.35


class VectorDBNotBuiltError(RuntimeError):
    pass


class FairyVectorDB:
    """Local vector database for ATU types and Motif-Index (TMI).

    Storage:
      - SQLite stores document text + metadata
      - Two HNSW indices store vectors for collections: 'atu' and 'motif'

    Embeddings are always generated by Ollama using the configured embedding model.
    """

    def __init__(self, *, paths: Optional[VectorDBPaths] = None):
        import threading

        self.paths = paths or default_paths()
        self._conn = None
        self._conn_lock = threading.Lock()
        self._meta: Dict[str, Any] = {}
        self._atu_index: Optional[HNSWIndex] = None
        self._motif_index: Optional[HNSWIndex] = None

    # -------------------------
    # Build
    # -------------------------

    def build_from_csvs(self, *, sources: SourcePaths, config: BuildConfig) -> None:
        """Build (or rebuild) the vector database from the two CSV sources."""

        self.paths.root_dir.mkdir(parents=True, exist_ok=True)

        print("[vector-db] Loading CSV sources...")

        # (Re)create sqlite content
        conn = connect(self.paths.sqlite_path)
        ensure_schema(conn)

        # Load records
        atu_records = list(iter_atu_records(sources.atu_csv))
        motif_records = list(iter_motif_records(sources.motif_csv))

        print(f"[vector-db] Parsed ATU records: {len(atu_records)}")
        print(f"[vector-db] Parsed Motif records: {len(motif_records)}")

        upsert_documents(conn, atu_records)
        upsert_documents(conn, motif_records)
        conn.commit()

        print("[vector-db] Wrote documents to SQLite")

        # Determine embedding dimension
        sample_text = (atu_records[0].text if atu_records else None) or (motif_records[0].text if motif_records else None)
        if not sample_text:
            raise ValueError("No documents found in the provided CSVs")

        sample_vec = ollama_embed(
            base_url=config.ollama_base_url,
            model=config.embedding_model,
            inputs=[sample_text],
            timeout_s=600.0,
        )[0]
        dim = len(sample_vec)
        if dim <= 0:
            raise ValueError("Embedding model returned empty vectors")

        # Build indices by iterating SQLite rows for stable integer IDs.
        # Note: we rely on SQLite autoincrement IDs for correspondence with HNSW labels.
        atu_count = count_collection(conn, "atu")
        motif_count = count_collection(conn, "motif")
        total = atu_count + motif_count

        print(f"[vector-db] SQLite counts: atu={atu_count} motif={motif_count} total={total}")
        print(f"[vector-db] Embedding model: {config.embedding_model} (dim={dim})")

        meta = {
            "embedding_model": config.embedding_model,
            "ollama_base_url": config.ollama_base_url,
            "dim": dim,
            "counts": {"atu": atu_count, "motif": motif_count, "total": total},
            "hnsw": {
                "space": config.hnsw.space,
                "ef_construction": config.hnsw.ef_construction,
                "m": config.hnsw.m,
                "ef_search": config.hnsw.ef_search,
            },
        }

        # Build ATU index
        print("[vector-db] Building ATU HNSW index...")
        self._build_index_for_collection(
            conn=conn,
            collection="atu",
            index_path=self.paths.atu_index_path,
            dim=dim,
            config=config,
            max_elements=atu_count,
        )

        # Build Motif index
        print("[vector-db] Building Motif HNSW index...")
        self._build_index_for_collection(
            conn=conn,
            collection="motif",
            index_path=self.paths.motif_index_path,
            dim=dim,
            config=config,
            max_elements=motif_count,
        )

        # Save meta
        self.paths.meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
        print("[vector-db] Wrote meta.json")
        print(f"[vector-db] Build complete: {self.paths.root_dir}")
        conn.close()

    def _build_index_for_collection(
        self,
        *,
        conn,
        collection: str,
        index_path: Path,
        dim: int,
        config: BuildConfig,
        max_elements: int,
    ) -> None:
        from .sqlite_store import iter_collection

        try:
            from tqdm import tqdm  # type: ignore
        except Exception:  # pragma: no cover
            tqdm = None  # type: ignore

        idx = HNSWIndex(dim=dim, config=config.hnsw)
        idx.init(max_elements=max(1, max_elements))

        batch_texts: List[str] = []
        batch_ids: List[int] = []

        pbar = None
        if tqdm is not None:
            pbar = tqdm(total=max_elements, desc=f"Embedding+Indexing {collection}")

        def flush() -> None:
            if not batch_texts:
                return
            vecs = ollama_embed(
                base_url=config.ollama_base_url,
                model=config.embedding_model,
                inputs=batch_texts,
                timeout_s=600.0,
            )
            idx.add(vectors=vecs, ids=batch_ids)
            if pbar is not None:
                pbar.update(len(batch_texts))
            batch_texts.clear()
            batch_ids.clear()

        for doc_id, rec in iter_collection(conn, collection):
            batch_ids.append(doc_id)
            batch_texts.append(rec.text)
            if len(batch_texts) >= config.embed_batch_size:
                flush()
        flush()

        if pbar is not None:
            pbar.close()

        idx.save(index_path)

    # -------------------------
    # Load
    # -------------------------

    def load(self) -> None:
        if not self.paths.meta_path.exists():
            raise VectorDBNotBuiltError(
                f"Vector DB metadata not found at {self.paths.meta_path}. Build it first."
            )

        self._meta = json.loads(self.paths.meta_path.read_text(encoding="utf-8"))
        dim = int(self._meta.get("dim") or 0)
        if dim <= 0:
            raise VectorDBNotBuiltError("Invalid meta.json (missing dim)")

        conn = connect(self.paths.sqlite_path)
        ensure_schema(conn)
        self._conn = conn

        # Initialize + load indices.
        # We set max_elements using current DB counts (safe upper bound for load).
        atu_count = count_collection(conn, "atu")
        motif_count = count_collection(conn, "motif")

        hnsw_cfg = HNSWConfig(
            space=str(self._meta.get("hnsw", {}).get("space", "cosine")),
            ef_construction=int(self._meta.get("hnsw", {}).get("ef_construction", 200)),
            m=int(self._meta.get("hnsw", {}).get("m", 32)),
            ef_search=int(self._meta.get("hnsw", {}).get("ef_search", 64)),
        )

        atu = HNSWIndex(dim=dim, config=hnsw_cfg)
        atu.load(self.paths.atu_index_path, max_elements=max(1, atu_count))
        motif = HNSWIndex(dim=dim, config=hnsw_cfg)
        motif.load(self.paths.motif_index_path, max_elements=max(1, motif_count))

        self._atu_index = atu
        self._motif_index = motif

    # -------------------------
    # Query
    # -------------------------

    def detect(self, *, text: str, config: QueryConfig) -> Dict[str, Any]:
        """Detect likely ATU types and motifs present in the story text."""

        self._require_loaded()

        chunks = chunk_text(text, config=config.chunking)
        if not chunks:
            return {"atu": [], "motifs": [], "chunks": 0}

        # Embed chunks in batches
        chunk_vectors: List[List[float]] = []
        batch: List[str] = []
        for c in chunks:
            batch.append(c)
            if len(batch) >= 16:
                chunk_vectors.extend(
                    ollama_embed(
                        base_url=config.ollama_base_url,
                        model=config.embedding_model,
                        inputs=batch,
                        timeout_s=600.0,
                    )
                )
                batch.clear()
        if batch:
            chunk_vectors.extend(
                ollama_embed(
                    base_url=config.ollama_base_url,
                    model=config.embedding_model,
                    inputs=batch,
                    timeout_s=600.0,
                )
            )

        atu_scores = self._search_collection(
            vectors=chunk_vectors,
            collection="atu",
            top_k=config.top_k,
            min_similarity=config.atu_min_similarity,
        )
        motif_scores = self._search_collection(
            vectors=chunk_vectors,
            collection="motif",
            top_k=config.top_k,
            min_similarity=config.motif_min_similarity,
        )

        return {
            "atu": atu_scores,
            "motifs": motif_scores,
            "chunks": len(chunks),
            "embedding_model": config.embedding_model,
        }

    def _search_collection(
        self,
        *,
        vectors: Sequence[Sequence[float]],
        collection: str,
        top_k: int,
        min_similarity: float,
    ) -> List[Dict[str, Any]]:
        idx = self._get_index(collection)
        assert self._conn is not None

        best: Dict[int, float] = {}
        for vec in vectors:
            ids, distances = idx.knn(vector=vec, k=top_k)
            for doc_id, dist in zip(ids, distances):
                sim = _cosine_distance_to_similarity(dist)
                if sim < min_similarity:
                    continue
                prev = best.get(int(doc_id))
                if prev is None or sim > prev:
                    best[int(doc_id)] = sim

        # Fetch doc records and sort
        id_list = sorted(best.keys())
        with self._conn_lock:
            docs = fetch_by_ids(self._conn, id_list)

        scored: List[Dict[str, Any]] = []
        for doc_id, sim in best.items():
            rec = docs.get(int(doc_id))
            if not rec:
                continue
            scored.append(
                {
                    "doc_key": rec.doc_key,
                    "similarity": float(sim),
                    "metadata": rec.metadata,
                    "text": rec.text,
                }
            )

        scored.sort(key=lambda x: x["similarity"], reverse=True)
        return scored

    def _get_index(self, collection: str) -> HNSWIndex:
        self._require_loaded()
        if collection == "atu":
            if self._atu_index is None:
                raise VectorDBNotBuiltError("ATU index not loaded")
            return self._atu_index
        if collection == "motif":
            if self._motif_index is None:
                raise VectorDBNotBuiltError("Motif index not loaded")
            return self._motif_index
        raise ValueError(f"Unknown collection: {collection}")

    def _require_loaded(self) -> None:
        if self._conn is None or self._atu_index is None or self._motif_index is None:
            raise VectorDBNotBuiltError("Vector DB not loaded. Call load() first.")


__all__ = [
    "FairyVectorDB",
    "VectorDBPaths",
    "BuildConfig",
    "QueryConfig",
    "SourcePaths",
]
