from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-4B"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# parsing thinking content
try:
    # rindex finding 151668 (</think>)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print("thinking content:", thinking_content)
print("content:", content)

# ================= å¯¼å‡ºä¸ºOllamaæ ¼å¼ =================
import os
from pathlib import Path
from unsloth import FastLanguageModel

OUTPUT_DIR = "./models/exported"
OLLAMA_NAME = "qwen3-4b-baseline"

print("\n" + "=" * 60)
print("å¼€å§‹å¯¼å‡ºä¸ºOllamaæ ¼å¼")
print("=" * 60)

# 1. å…ˆä¿å­˜æ¨¡å‹å’Œtokenizer
print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {OUTPUT_DIR}...")
os.makedirs(OUTPUT_DIR, exist_ok=True)
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"âœ… æ¨¡å‹å·²ä¿å­˜")

# 2. ä½¿ç”¨unslothè½¬æ¢ä¸ºGGUF
print(f"ğŸ”„ æ­£åœ¨è½¬æ¢ä¸ºGGUFæ ¼å¼...")
try:
    # ç”¨unslothåŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ï¼ˆä»æœ¬åœ°è·¯å¾„ï¼‰
    export_model, export_tokenizer = FastLanguageModel.from_pretrained(
        model_name=OUTPUT_DIR,  # ä»æœ¬åœ°è·¯å¾„åŠ è½½
        max_seq_length=2048,
        load_in_4bit=False,  # å·²ç»åŠ è½½çš„æ¨¡å‹ä¸éœ€è¦å†é‡åŒ–
    )
    
    # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
    FastLanguageModel.for_inference(export_model)
    
    # å¯¼å‡ºä¸ºGGUF
    export_model.save_pretrained_gguf(
        OUTPUT_DIR,
        export_tokenizer,
        quantization_method="q4_k_m",
    )
    
    print(f"âœ… GGUFæ¨¡å‹å·²å¯¼å‡ºåˆ°: {OUTPUT_DIR}")
    
    # 3. åˆ›å»ºModelfileå¹¶å¯¼å…¥Ollama
    output_path = Path(OUTPUT_DIR)
    gguf_files = list(output_path.glob("*.gguf"))
    
    if gguf_files:
        gguf_file = gguf_files[0]
        modelfile_path = output_path / "Modelfile"
        modelfile_content = f"""FROM {gguf_file.absolute()}

# Model exported from Qwen3-4B
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER top_k 20
"""
        modelfile_path.write_text(modelfile_content, encoding="utf-8")
        print(f"âœ… Modelfileå·²åˆ›å»º: {modelfile_path}")
        
        # å¯¼å…¥åˆ°Ollama
        import subprocess
        print(f"ğŸ“¦ æ­£åœ¨å¯¼å…¥åˆ°Ollama (æ¨¡å‹åç§°: {OLLAMA_NAME})...")
        try:
            subprocess.run(
                ["ollama", "create", OLLAMA_NAME, "-f", str(modelfile_path)],
                check=True,
            )
            print(f"âœ… æ¨¡å‹å·²æˆåŠŸå¯¼å…¥åˆ°Ollama: {OLLAMA_NAME}")
            print(f"ğŸš€ ç°åœ¨å¯ä»¥ä½¿ç”¨: ollama run {OLLAMA_NAME}")
        except FileNotFoundError:
            print(f"âš ï¸  æ‰¾ä¸åˆ°ollamaå‘½ä»¤ï¼Œè¯·æ‰‹åŠ¨å¯¼å…¥:")
            print(f"   ollama create {OLLAMA_NAME} -f {modelfile_path}")
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸  å¯¼å…¥å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å¯¼å…¥:")
            print(f"   ollama create {OLLAMA_NAME} -f {modelfile_path}")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°GGUFæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥å¯¼å‡ºè¿‡ç¨‹")
        
except Exception as e:
    print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
